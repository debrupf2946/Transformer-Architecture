{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab2d0b31-6299-477b-a130-08959fe65e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "import math\n",
    "\n",
    "def scaled_dot_product(q,k,v,mask=None):\n",
    "    scaled=torch.matmul(q,k.transpose(-1,-2))/math.sqrt(k.shape[-1])\n",
    "    if mask==True:\n",
    "        mask=torch.full(scaled.size(),float('-inf'))\n",
    "        mask=torch.triu(mask,diagonal=1)\n",
    "        scaled=scaled+mask\n",
    "        # print(mask)\n",
    "        # print(scaled)\n",
    "    attention=F.softmax(scaled,dim=-1)\n",
    "    values=torch.matmul(attention,v)\n",
    "    return values,attention\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,input_dim,model_dim,num_head):\n",
    "        super(MultiHeadAttention, self).__init__() \n",
    "        self.input_dim=input_dim\n",
    "        self.model_dim=model_dim\n",
    "        self.num_head=num_head\n",
    "        self.head_dim=model_dim//num_head\n",
    "        self.qkv_layer=nn.Linear(input_dim,3*model_dim)\n",
    "        self.linear=nn.Linear(model_dim,input_dim)\n",
    "    def forward(self,x,mask=None):\n",
    "        self.mask=mask\n",
    "        batch_dim,seq_length,input_dim=x.size()\n",
    "        qkv=self.qkv_layer(x)\n",
    "        qkv=qkv.reshape(batch_dim,seq_length,self.num_head,3*self.head_dim)\n",
    "        qkv=qkv.permute(0,2,1,3)\n",
    "        q,k,v=qkv.chunk(3,dim=-1)\n",
    "        values,attention=scaled_dot_product(q,k,v,mask=mask)\n",
    "        values=values.reshape(batch_size,seq_length,self.num_head*self.head_dim)\n",
    "        out=self.linear(values)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ea012dc5-7a6e-484c-8469-bac56ec1c755",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalisation(nn.Module):\n",
    "    def __init__(self,parameter_shape,epsilon):\n",
    "        super().__init__()\n",
    "        self.parameter_shape=parameter_shape\n",
    "        self.eps=epsilon\n",
    "        self.gamma=nn.Parameter(torch.ones(parameter_shape))\n",
    "        self.beta=nn.Parameter(torch.zeros(parameter_shape))\n",
    "    def forward(self,input):\n",
    "        dim=[-i-1 for i in range(len(self.parameter_shape))]\n",
    "        mean=input.mean(dim=dim,keepdim=True)\n",
    "        var=((input-mean)**2).mean(dim=dim,keepdim=True)\n",
    "        std=(var+self.eps).sqrt()\n",
    "        y=(input-mean)/std\n",
    "        out=self.gamma*y + self.beta\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "60e3dbb9-32dd-4067-9ed2-7c60ac13692e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardLayer(nn.Module):\n",
    "    def __init__(self,model_dim,hidden_dim,dropout):\n",
    "        super(FeedForwardLayer,self).__init__()\n",
    "        self.linear1=nn.Linear(model_dim,hidden_dim)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.linear2=nn.Linear(hidden_dim,model_dim)\n",
    "        self.dropout=nn.Dropout(p=dropout)\n",
    "    def forward(self,x):\n",
    "        x=self.linear1(x)\n",
    "        x=self.relu(x)\n",
    "        x=self.linear2(x)\n",
    "        x=self.dropout(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9675f40f-5403-456a-a2a1-7477fb0d9a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self,model_dim,hidden_dim,num_head,dropout):\n",
    "        super(EncoderLayer,self).__init__()\n",
    "        self.model_dim=model_dim\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.num_head=num_head\n",
    "        self.dropout=dropout\n",
    "        self.attention=MultiHeadAttention(model_dim,model_dim,num_head)\n",
    "        self.norm1=LayerNormalisation([model_dim],1e-5)\n",
    "        self.dropout1=nn.Dropout(p=dropout)\n",
    "        self.ffn=FeedForwardLayer(model_dim,hidden_dim,dropout)\n",
    "        self.norm2=LayerNormalisation([model_dim],1e-5)\n",
    "        self.dropout2=nn.Dropout(p=dropout)\n",
    "    def forward(self,x):\n",
    "        x_res=x\n",
    "        print(\"------- ATTENTION 1 ------\")\n",
    "        x=self.attention(x)\n",
    "        print(\"------- DROPOUT 1 ------\")\n",
    "        x=self.dropout1(x)\n",
    "        print(\"------- ADD AND LAYER NORMALIZATION 1 ------\")\n",
    "        x=self.norm1(x+x_res)\n",
    "        x_res=x\n",
    "        x=self.ffn(x)\n",
    "        print(\"------- DROPOUT 2 ------\")\n",
    "        x=self.dropout2(x)\n",
    "        print(\"------- ADD AND LAYER NORMALIZATION 2 ------\")\n",
    "        x=self.norm2(x+x_res)\n",
    "        return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c777799-07fa-4960-a74c-f5a3266371f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self,model_dim,hidden_dim,num_head,dropout,num_layers):\n",
    "        super().__init__()\n",
    "        self.layers=nn.Sequential(*[EncoderLayer(model_dim,hidden_dim,num_head,dropout) for _ in range(num_layers)])\n",
    "    def forward(self,x):\n",
    "        x=self.layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "d48e1d78-dc34-4651-bcac-3a531e3bf577",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadCrossAttention(nn.Module):\n",
    "    def __init__(self,model_dim,num_heads):\n",
    "        super().__init__()\n",
    "        self.model_dim=model_dim\n",
    "        self.num_heads=num_heads\n",
    "        self.head_dim=model_dim//num_heads\n",
    "        self.kv_layer=nn.Linear(model_dim,2*model_dim)\n",
    "        self.q_layer=nn.Linear(model_dim,model_dim)\n",
    "        self.linear=nn.Linear(model_dim,model_dim)\n",
    "    def forward(self,x,y):\n",
    "        batch_size,seq_length,model_dim=x.size()\n",
    "        kv=self.kv_layer(x)\n",
    "        q=self.q_layer(y)\n",
    "        kv=kv.reshape(batch_size,seq_length,self.num_heads,2*self.head_dim)\n",
    "        q=q.reshape(batch_size,seq_length,self.num_heads,self.head_dim)\n",
    "        kv=kv.permute(0,2,1,3)\n",
    "        q=q.permute(0,2,1,3)\n",
    "        k,v=kv.chunk(2,dim=-1)\n",
    "        values,attention=scaled_dot_product(q,k,v,mask=True)\n",
    "        values=values.reshape(batch_size,seq_length,model_dim)\n",
    "        out=self.linear(values)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "63f8e929-f1e8-471d-814b-adefbeb28529",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self,model_dim,hidden_dim,num_heads,dropout):\n",
    "        super(DecoderLayer,self).__init__()\n",
    "        self.model_dim=model_dim\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.num_heads=num_head\n",
    "        self.dropout=dropout\n",
    "        self.attention=MultiHeadAttention(model_dim,model_dim,num_heads)\n",
    "        self.dropout1=nn.Dropout(p=dropout)\n",
    "        self.norm1=LayerNormalisation([model_dim],1e-5)\n",
    "        self.encoder_decoder_attention=MultiheadCrossAttention(model_dim,num_heads)\n",
    "        self.dropout2=nn.Dropout(p=dropout)\n",
    "        self.norm2=LayerNormalisation([model_dim],1e-5)\n",
    "        self.ffn=FeedForwardLayer(model_dim,model_dim,dropout)\n",
    "        self.dropout3=nn.Dropout(p=dropout)\n",
    "        self.norm3=LayerNormalisation([model_dim],1e-5)\n",
    "    def forward(self,x,y):\n",
    "        y_res=y\n",
    "        \n",
    "        print(\"MASKED SELF ATTENTION\")\n",
    "        y=self.attention(y,mask=True)\n",
    "        print(\"DROP OUT 1\")\n",
    "        y=self.dropout1(y)\n",
    "        print(\"ADD + LAYER NORMALIZATION 1\")\n",
    "        y=self.norm1(y+y_res)\n",
    "        \n",
    "        y_res=y\n",
    "        \n",
    "        print(\"CROSS ATTENTION\")\n",
    "        y=self.encoder_decoder_attention(x,y)\n",
    "        print(\"DROP OUT 2\") \n",
    "        y=self.dropout2(y)\n",
    "        print(\"ADD + LAYER NORMALIZATION 2\")\n",
    "        y=self.norm2(y+y_res)\n",
    "        \n",
    "        print(\"FEED FORWARD 1\")\n",
    "        y=self.ffn(y)\n",
    "        print(\"DROP OUT 3\")\n",
    "        y=self.dropout3(y)\n",
    "        print(\"ADD + LAYER NORMALIZATION 3\")\n",
    "        y=self.norm3(y+y_res)\n",
    "        return y\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b3e393e3-24fe-4a49-8128-cf2eefbb6b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequentialDecoder(nn.Sequential):\n",
    "    def forward(self,*inputs):\n",
    "        x,y=inputs\n",
    "        for module in self._modules.values():\n",
    "            y=module(x,y)\n",
    "            return y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "64365669-6931-413b-a3f3-8f13daf177c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,model_dim,hidden_dim,num_head,dropout,num_layers=1):\n",
    "        super().__init__()\n",
    "        self.layers=SequentialDecoder(*[DecoderLayer(model_dim,hidden_dim,num_head,dropout) for _ in range(num_layers)])\n",
    "    def forward(self,x,y):\n",
    "        y = self.layers(x, y)\n",
    "        return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9efd6ce4-14c4-45d2-a705-db47fe03cad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dim=512\n",
    "hidden_dim=2048\n",
    "num_head=8\n",
    "dropout=0.1\n",
    "num_layers=5\n",
    "seq_length=200\n",
    "batch_size=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "23c3d3c7-9b72-4589-8aed-5bd532441fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder=Encoder(model_dim,hidden_dim,num_head,dropout,num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "22e0c1c3-8cf5-46a1-838a-8403746dbc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=torch.randn((batch_size,seq_length,model_dim))\n",
    "y=torch.randn((batch_size,seq_length,model_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ebb1898f-0b95-4731-b355-9895210d3361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 200, 512])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "103b885a-f9ad-4d73-b78e-f790d32df618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- ATTENTION 1 ------\n",
      "------- DROPOUT 1 ------\n",
      "------- ADD AND LAYER NORMALIZATION 1 ------\n",
      "------- DROPOUT 2 ------\n",
      "------- ADD AND LAYER NORMALIZATION 2 ------\n",
      "------- ATTENTION 1 ------\n",
      "------- DROPOUT 1 ------\n",
      "------- ADD AND LAYER NORMALIZATION 1 ------\n",
      "------- DROPOUT 2 ------\n",
      "------- ADD AND LAYER NORMALIZATION 2 ------\n",
      "------- ATTENTION 1 ------\n",
      "------- DROPOUT 1 ------\n",
      "------- ADD AND LAYER NORMALIZATION 1 ------\n",
      "------- DROPOUT 2 ------\n",
      "------- ADD AND LAYER NORMALIZATION 2 ------\n",
      "------- ATTENTION 1 ------\n",
      "------- DROPOUT 1 ------\n",
      "------- ADD AND LAYER NORMALIZATION 1 ------\n",
      "------- DROPOUT 2 ------\n",
      "------- ADD AND LAYER NORMALIZATION 2 ------\n",
      "------- ATTENTION 1 ------\n",
      "------- DROPOUT 1 ------\n",
      "------- ADD AND LAYER NORMALIZATION 1 ------\n",
      "------- DROPOUT 2 ------\n",
      "------- ADD AND LAYER NORMALIZATION 2 ------\n"
     ]
    }
   ],
   "source": [
    "out=encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0c4890d4-d711-4d53-938c-f2a46e391c6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-9.5982e-01,  3.6257e+00, -2.9835e-01,  ..., -9.8806e-01,\n",
       "          -2.0667e-01,  6.8550e-01],\n",
       "         [-9.9231e-01, -6.8306e-01,  8.2421e-01,  ...,  1.5110e+00,\n",
       "           5.9254e-01,  8.6756e-01],\n",
       "         [-5.4171e-01,  1.0487e+00,  8.6307e-01,  ...,  1.5231e+00,\n",
       "           8.1216e-01,  2.8084e-01],\n",
       "         ...,\n",
       "         [-1.3760e+00,  2.0386e-01,  1.1781e+00,  ...,  5.0603e-01,\n",
       "          -1.3255e+00, -1.5862e+00],\n",
       "         [-1.1459e-01,  1.3386e+00,  9.9571e-01,  ..., -8.8185e-01,\n",
       "           7.6200e-01,  5.4307e-01],\n",
       "         [ 3.4250e-01,  9.9287e-01,  5.0129e-01,  ..., -8.7733e-01,\n",
       "          -1.3725e+00,  1.1900e+00]],\n",
       "\n",
       "        [[ 2.1631e-01,  3.9607e-01,  3.0075e+00,  ...,  4.5226e-01,\n",
       "          -9.1189e-01,  1.4128e+00],\n",
       "         [ 6.4642e-01,  3.5981e-01,  7.7689e-01,  ..., -1.4002e-01,\n",
       "           6.4884e-01, -6.3119e-01],\n",
       "         [-1.9533e-01,  9.8765e-01,  1.5618e-01,  ...,  3.0139e-01,\n",
       "          -8.6092e-01, -3.5307e-01],\n",
       "         ...,\n",
       "         [-1.3435e+00,  1.7237e+00,  9.1195e-02,  ...,  1.2447e+00,\n",
       "          -6.4595e-01, -2.6069e-01],\n",
       "         [-1.5940e+00,  2.0941e+00,  1.8863e-01,  ...,  8.9034e-01,\n",
       "           5.5251e-01,  1.4633e+00],\n",
       "         [ 5.5530e-01, -6.6536e-01, -8.2436e-01,  ...,  1.5430e+00,\n",
       "          -1.8752e-01,  1.4296e+00]],\n",
       "\n",
       "        [[-2.1669e-01, -8.6285e-01,  1.2275e+00,  ..., -7.9134e-03,\n",
       "          -1.2718e+00, -9.5719e-02],\n",
       "         [-1.1148e+00,  1.0773e+00,  3.4948e-01,  ..., -5.0240e-01,\n",
       "           7.7701e-01,  5.4375e-01],\n",
       "         [ 3.1484e-01,  4.8187e-01, -3.7462e-01,  ..., -5.0520e-01,\n",
       "           6.2143e-02,  9.0285e-01],\n",
       "         ...,\n",
       "         [ 1.5125e+00,  4.4052e-01,  4.1598e-01,  ...,  9.3705e-01,\n",
       "          -1.5863e+00,  1.1609e+00],\n",
       "         [-1.6544e+00, -2.6558e-01,  2.5346e-01,  ..., -8.0544e-01,\n",
       "          -4.5239e-01, -1.0830e-03],\n",
       "         [-1.8198e+00, -8.0058e-01,  8.7513e-01,  ...,  1.0544e+00,\n",
       "          -1.5185e+00, -2.1815e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-2.5010e-01, -2.7170e-01,  8.3586e-01,  ...,  1.8785e+00,\n",
       "          -2.0432e+00,  1.5447e+00],\n",
       "         [ 7.2399e-01, -4.7429e-02,  6.4645e-01,  ..., -2.4408e-01,\n",
       "           2.8150e-02, -1.7176e+00],\n",
       "         [-8.0932e-03, -2.5624e-01,  2.0938e-01,  ..., -6.0065e-01,\n",
       "           8.0080e-01,  1.6296e+00],\n",
       "         ...,\n",
       "         [ 1.0813e+00,  1.5491e-01,  2.0381e+00,  ..., -8.2575e-01,\n",
       "          -6.2498e-01,  6.3503e-01],\n",
       "         [-8.2580e-01,  1.3866e+00,  4.2062e-01,  ..., -1.2877e+00,\n",
       "          -1.4094e+00, -7.3754e-01],\n",
       "         [-5.7692e-01,  1.5086e-01,  1.4387e+00,  ..., -3.0303e-01,\n",
       "           6.1654e-01, -2.0653e-01]],\n",
       "\n",
       "        [[-9.0649e-01,  1.1585e+00,  2.3663e+00,  ..., -2.5675e-02,\n",
       "          -7.9600e-01,  1.8816e+00],\n",
       "         [ 1.6229e+00, -4.8705e-01,  1.6484e+00,  ..., -3.4039e-01,\n",
       "          -3.5825e-01,  7.3387e-01],\n",
       "         [ 9.9978e-01, -8.1173e-01, -1.3014e+00,  ...,  5.6177e-01,\n",
       "           6.6886e-01,  7.0597e-02],\n",
       "         ...,\n",
       "         [ 6.9904e-01, -8.1950e-01, -1.8291e-01,  ...,  2.8706e-01,\n",
       "          -1.6883e+00,  2.8944e-01],\n",
       "         [ 2.3915e+00,  7.9296e-02,  1.5154e+00,  ...,  3.0511e-01,\n",
       "           3.9419e-01,  1.7922e-01],\n",
       "         [-5.7494e-01,  7.0233e-01,  4.5118e-01,  ...,  7.9784e-01,\n",
       "          -9.1600e-01,  1.4901e+00]],\n",
       "\n",
       "        [[-2.5712e+00, -1.6493e+00,  2.2713e-01,  ..., -5.6136e-01,\n",
       "          -2.9471e+00, -7.8322e-02],\n",
       "         [ 8.0749e-01,  9.9440e-02,  9.0861e-01,  ...,  5.0149e-01,\n",
       "          -6.1041e-01, -6.3895e-01],\n",
       "         [ 1.3084e-01, -4.2067e-02,  1.2305e+00,  ...,  6.4836e-01,\n",
       "           8.6128e-02,  1.0838e+00],\n",
       "         ...,\n",
       "         [-2.3972e+00,  1.0390e+00,  9.8764e-01,  ...,  5.2154e-01,\n",
       "          -2.2417e+00, -3.2501e-01],\n",
       "         [-9.3606e-01,  3.6384e-01,  7.6234e-01,  ..., -1.1187e+00,\n",
       "          -9.8541e-02,  9.7121e-01],\n",
       "         [-1.2616e+00,  1.5006e+00,  1.0135e+00,  ...,  5.4863e-01,\n",
       "          -1.1126e-01,  1.1911e+00]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f2c77a8b-3cb6-43e3-b892-7a3941cb4af7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 200, 512])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "6dec5223-f4eb-4b24-954a-f97ade923806",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder=Decoder(model_dim,hidden_dim,num_head,dropout,num_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ff01bd72-5abf-4604-bed2-dfb32b1c6e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MASKED SELF ATTENTION\n",
      "DROP OUT 1\n",
      "ADD + LAYER NORMALIZATION 1\n",
      "CROSS ATTENTION\n",
      "DROP OUT 2\n",
      "ADD + LAYER NORMALIZATION 2\n",
      "FEED FORWARD 1\n",
      "DROP OUT 3\n",
      "ADD + LAYER NORMALIZATION 3\n"
     ]
    }
   ],
   "source": [
    "out=decoder(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "aa4cb34f-20b0-4481-91b6-4f30789077b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 200, 512])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac17eaba-9bf6-4f60-89d9-1aae3e6ec36b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
